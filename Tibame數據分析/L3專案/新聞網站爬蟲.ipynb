{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from time import sleep\n",
    "import os\n",
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = []\n",
    "url = 'https://money.udn.com/money/index'\n",
    "\n",
    "## 1.填寫headers\n",
    "headers = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36'}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#print(response.text)\n",
    "\n",
    "## 2.抓取即時區域所有新聞title以及新聞內容連結link，並將每篇新聞的連結link append到new_list內\n",
    "latest_story = soup.findAll('li', {'class':'latest story'})\n",
    "story_content = latest_story[0].findAll('div', {'class':'story__content'})\n",
    "#print(story_content)\n",
    "#print(\"====\")\n",
    "\n",
    "for all_a_tags in story_content:\n",
    "    a_tags = all_a_tags.findAll('a')\n",
    "    for i in a_tags:\n",
    "        print('title: {}, link: {}'.format(i.h3.text.strip(), 'https://money.udn.com'+i['href']))\n",
    "        new_list.append('https://money.udn.com'+i['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFolder(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print ('Error: Creating directory. ' +  directory)\n",
    "\n",
    "        \n",
    "def save_news(url):\n",
    "    ## 3.填寫headers\n",
    "    headers = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    sleep(1)\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    ## 4. 使用find_all去抓取每篇新聞的title(指定是哪個tag以及指定id)，將結果所產生之list放到new_title變數內\n",
    "    \n",
    "    new_title = soup.findAll('h1', {'id':'story_art_title'})\n",
    "    \n",
    "    save_path = './news/'+new_title[0].text.replace(':','').replace(\"?\", \"\").strip()\n",
    "\n",
    "    article_body = soup.find_all('section',{'id':'article_body'})\n",
    "    print('new title: {}'.format(new_title[0].text))\n",
    "\n",
    "    ## create folder\n",
    "    createFolder(save_path)\n",
    "    soup2 = BeautifulSoup(str(article_body[0]), 'html.parser')\n",
    "\n",
    "\n",
    "    ## save article contens in folder\n",
    "    with open(save_path+'/contents.txt', 'w', encoding='utf-8') as f:  \n",
    "        ## 5. 抓取文章內p tags，並將新聞內文寫入contents.txt檔案內\n",
    "        p_tags = soup2.findAll('p')\n",
    "        print('new content: ')\n",
    "        for i in p_tags:\n",
    "            print(i.text)\n",
    "            f.write(i.text)\n",
    "\n",
    "    ## save image in article        \n",
    "    \n",
    "    ## 6. 抓取文章內所有img tags，並將圖片下載並存入對應新聞資料夾下\n",
    "    print('save image: ')\n",
    "    img_tags = soup2.findAll('img')\n",
    "    for i in img_tags:\n",
    "        print(i['src'])\n",
    "        urlretrieve(i['src'], save_path+'/'+ i['title'].replace(':','').replace(\"?\", \"\").replace(\"/\", \"\").replace(\"\\\\\",'').strip() +'.jpg')\n",
    "    print('==========\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for new_url in new_list:\n",
    "    print(new_url)\n",
    "    save_news(new_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 優化版本\n",
    "- 針對最新的新聞頁面進行動態爬蟲"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from time import sleep\n",
    "import os\n",
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://money.udn.com/rank/ajax_newest/1001/0/{}'\n",
    "headers = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36'}\n",
    "new_list = []\n",
    "\n",
    "# 要搜尋的頁數\n",
    "for i in range(1,2):\n",
    "    response3 = requests.get(url.format(i), headers=headers)\n",
    "    soup4 = BeautifulSoup(response3.text, 'html.parser')\n",
    "    new_title = soup4.findAll('div', {'class':'story__content'})\n",
    "\n",
    "    for j in new_title:\n",
    "        print(\"title: {}, link: {}\".format(j.a[\"title\"], j.a[\"href\"]))\n",
    "        new_list.append(j['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFolder(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print ('Error: Creating directory. ' +  directory)\n",
    "        \n",
    "def save_news(url):    \n",
    "    ## 3.填寫headers\n",
    "    headers = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    sleep(1)\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    ## 4. 使用find_all去抓取每篇新聞的title(指定是哪個tag以及指定id)，將結果所產生之list放到new_title變數內\n",
    "    new_title = soup.findAll('h1', {'id':'story_art_title'})\n",
    "    save_path = './news/'+new_title[0].text.replace(':','').replace(\"?\", \"\").replace(\"/\", \"\").replace(\"\\\\\",'').strip()\n",
    "    article_body = soup.find_all('section',{'id':'article_body'})\n",
    "    print('new title: {}'.format(new_title[0].text))\n",
    "\n",
    "    ## create folder\n",
    "    createFolder(save_path)\n",
    "    soup2 = BeautifulSoup(str(article_body[0]), 'html.parser')\n",
    "\n",
    "    ## save article contens in folder\n",
    "    with open(save_path+'/contents.txt', 'w', encoding='utf-8') as f:  \n",
    "        ## 5. 抓取文章內p tags，並將新聞內文寫入contents.txt檔案內\n",
    "        p_tags = soup2.findAll('p')\n",
    "        print('new content: ')\n",
    "        for i in p_tags:\n",
    "            print(i.text)\n",
    "            f.write(i.text)\n",
    "\n",
    "    ## save image in article        \n",
    "    \n",
    "    ## 6. 抓取文章內所有img tags，並將圖片下載並存入對應新聞資料夾下\n",
    "    print('save image: ')\n",
    "    img_tags = soup2.findAll('img')\n",
    "    for i in img_tags:\n",
    "        print(i['src'])\n",
    "        urlretrieve(i['src'], save_path+'/'+ i['title'].replace(':','').replace(\"?\", \"\").replace(\"/\", \"\").replace(\"\\\\\",'').strip() +'.jpg')\n",
    "    print('==========\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for new_url in new_list:\n",
    "    print(new_url)\n",
    "    save_news(new_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
